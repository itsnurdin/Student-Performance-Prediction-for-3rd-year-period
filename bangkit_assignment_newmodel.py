# -*- coding: utf-8 -*-
"""Bangkit Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/itsnurdin/Student-Performance-Prediction-for-3rd-year-period/blob/master/Bangkit_Project1.ipynb

## Install and Import Library
"""

!pip install -q git+https://github.com/tensorflow/docs

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas.util.testing as tm

import tensorflow_docs as tfdocs
import tensorflow_docs.plots
import tensorflow_docs.modeling
from google.colab import drive
drive.mount('/content/drive')

"""## Read and Inspect (a little) Dataset"""

df_math = pd.read_csv("/content/drive/My Drive/DataPrepAssigment5/student-mat.csv",delimiter=';')

df_math.head()

df_math.info()

df_math.isna().sum()

df_math.isnull().sum()

# # Fill NaN with ' '
df_math['absences'] = df_math['absences'].fillna(' ')
# Full NaN with  the mean of the column
df_math['G1'] = df_math['G1'].fillna(' ')
df_math['G2'] = df_math['G2'].fillna(' ')

df_math.describe()

def getDuplicateColumns(df_math):
    '''
    Get a list of duplicate columns.
    It will iterate over all the columns in dataframe and find the columns whose contents are duplicate.
    :param df: Dataframe object
    :return: List of columns whose contents are duplicates.
    '''
    duplicateColumnNames = set()
    # Iterate over all the columns in dataframe
    for x in range(df_math.shape[1]):
        # Select column at xth index.
        col = df_math.iloc[:, x]
        # Iterate over all the columns in DataFrame from (x+1)th index till end
        for y in range(x + 1, df_math.shape[1]):
            # Select column at yth index.
            otherCol = df_math.iloc[:, y]
            # Check if two columns at x 7 y index are equal
            if col.equals(otherCol):
                duplicateColumnNames.add(df_math.columns.values[y])
    return list(duplicateColumnNames)

# Get list of duplicate columns
duplicateColumnNames = getDuplicateColumns(df_math)
print('Duplicate Columns are as follows')
for col in duplicateColumnNames:
    print('Column name : ', col)

X, y = df_math.drop(['G3'],axis=1), df_math['G3'] ## Split the features and label

cat_col = X.drop(['G1','G2','absences'],axis=1).columns ## Take all the category column
for col in cat_col:
  X[col] = X[col].astype('category') ## Change their type into categorical so that we can use pd.get_dummies

X = pd.get_dummies(X) ## One Hot Encoding all categorical features

num_col = ['G1','G2','absences'] ## Take all numerical columns
for col in num_col:
  sc = StandardScaler()
  X[col] =sc.fit_transform(X[col].values.reshape(-1,1)) ## Standardize them

# # Fill NaN with ' '
#X['absences'] = X['absences'].fillna(' ')
# Full NaN with  the mean of the column
#X['G1'] = X['G1'].fillna(' ')
#X['G2'] = X['G2'].fillna(' ')

# Check the index values
X.index.values

def build_model():
  model = keras.Sequential([
    layers.Dense(8, activation='relu', input_shape=[X.shape[1]]),
    layers.Dense(8, activation='relu'),
    layers.Dense(1)
  ])

  optimizer = tf.keras.optimizers.Adam(lr=0.001)

  model.compile(loss='mse',
                optimizer=optimizer,
                metrics=['mae', 'mse'])
  return model

model = build_model()
X_train, X_test, y_train, y_test = train_test_split(X,y)

EPOCHS = 1000

history = model.fit(
  X_train, y_train,
  epochs=EPOCHS, validation_split = 0.2, verbose=0,
  callbacks=[tfdocs.modeling.EpochDots()])

hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()

plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)

plotter.plot({'Basic': history}, metric = "mae")
plt.ylim([0, 10])
plt.ylabel('MAE [G3]')

plotter.plot({'Basic': history}, metric = "mse")
plt.ylim([0, 20])
plt.ylabel('MSE [G2^2]')

model = build_model()

# The patience parameter is the amount of epochs to check for improvement
early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)

early_history = model.fit(X_train, y_train, 
                    epochs=EPOCHS, validation_split = 0.2, verbose=0, 
                    callbacks=[early_stop, tfdocs.modeling.EpochDots()])

plotter.plot({'Early Stopping': early_history}, metric = "mae")
plt.ylim([0, 10])
plt.ylabel('MAE [G3]')

test_predictions = model.predict(X_test).flatten()

a = plt.axes(aspect='equal')
plt.scatter(y_test, test_predictions)
plt.xlabel('True Values [MPG]')
plt.ylabel('Predictions [MPG]')
lims = [0, 20]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims, lims)

loss, mae, mse = model.evaluate(X_test, y_test, verbose=2)

print("Testing set Mean Abs Error: {:5.2f} G3 Score".format(mae))